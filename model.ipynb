{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import math\n",
    "from math import sqrt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "# from tqdm.auto import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "# from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from transformers import AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from difflib import SequenceMatcher\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import contextlib\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import timm\n",
    "from IPython.display import display\n",
    "from pycocotools.coco import COCO\n",
    "import shutil\n",
    "import warnings\n",
    "# Suppress FutureWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "try:\n",
    "    from rouge_score import rouge_scorer\n",
    "    import pycocoevalcap\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find ... installing it.\")\n",
    "    !pip install rouge_score\n",
    "    !pip install pycocoevalcap\n",
    "    from pycocoevalcap.bleu.bleu import Bleu\n",
    "    from pycocoevalcap.meteor.meteor import Meteor\n",
    "    from pycocoevalcap.rouge.rouge import Rouge\n",
    "    from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "# try:\n",
    "#     import gradio as gr\n",
    "# except:\n",
    "#     print(\"[INFO] Couldn't find gradio... installing it.\")\n",
    "#     !pip install gradio\n",
    "#     import gradio as gr\n",
    "\n",
    "# try:\n",
    "#     from torchinfo import summary\n",
    "# except:\n",
    "#     print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
    "#     !pip install -q torchinfo\n",
    "#     from torchinfo import summary\n",
    "\n",
    "try:\n",
    "    from helper_functions import download_data, set_seeds\n",
    "except:\n",
    "    # Get the going_modular scripts\n",
    "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
    "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
    "    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n",
    "    !rm -rf pytorch-deep-learning\n",
    "    from helper_functions import download_data, set_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_caption(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "    return f\"<bos> {text} <eos>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_flickr(input_directory):\n",
    "\n",
    "    # Đường dẫn tới các tập tin dữ liệu\n",
    "    captions_path = os.path.join(input_directory, \"captions.txt\")\n",
    "    image_path = os.path.join(input_directory, \"Images/flickr30k_images\")\n",
    "\n",
    "    # Đọc file captions.txt\n",
    "    data = pd.read_csv(captions_path, sep=',', header=None, names=['image', 'caption'], on_bad_lines='skip')\n",
    "\n",
    "    # Loại bỏ hàng đầu tiên nếu là tiêu đề\n",
    "    data = data[1:]\n",
    "    data = data.dropna()\n",
    "\n",
    "    # Thêm đường dẫn đầy đủ cho các ảnh\n",
    "    data['image'] = data['image'].apply(lambda x: os.path.join(image_path, x))\n",
    "\n",
    "    # Đặt lại chỉ số sau khi xử lý\n",
    "    data = data.reset_index(drop=True)\n",
    "\n",
    "    # Preprocess captions (nếu cần)\n",
    "    data['caption'] = data['caption'].apply(preprocess_caption)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def create_data_coco(input_directory=\"/kaggle/input/coco-2017-dataset\"):\n",
    "\n",
    "    # Đường dẫn dữ liệu\n",
    "    train_images = os.path.join(input_directory, \"train2017\")\n",
    "    val_images = os.path.join(input_directory, \"val2017\")\n",
    "    test_images = os.path.join(input_directory, \"test2017\")  # Test không có chú thích\n",
    "    train_annotations = os.path.join(input_directory, \"annotations/captions_train2017.json\")\n",
    "    val_annotations = os.path.join(input_directory, \"annotations/captions_val2017.json\")\n",
    "\n",
    "    # Xử lý train\n",
    "    coco_train = COCO(train_annotations)\n",
    "    train_data = []\n",
    "    for image_id in coco_train.getImgIds():\n",
    "        img_info = coco_train.loadImgs(image_id)[0]\n",
    "        captions = coco_train.loadAnns(coco_train.getAnnIds(imgIds=image_id))\n",
    "        for caption in captions:\n",
    "            train_data.append({\"image\": f\"{train_images}/{img_info['file_name']}\", \"caption\": caption['caption']})\n",
    "    train_df = pd.DataFrame(train_data)\n",
    "\n",
    "    # Xử lý val\n",
    "    coco_val = COCO(val_annotations)\n",
    "    val_data = []\n",
    "    for image_id in coco_val.getImgIds():\n",
    "        img_info = coco_val.loadImgs(image_id)[0]\n",
    "        captions = coco_val.loadAnns(coco_val.getAnnIds(imgIds=image_id))\n",
    "        for caption in captions:\n",
    "            val_data.append({\"image\": f\"{val_images}/{img_info['file_name']}\", \"caption\": caption['caption']})\n",
    "    val_df = pd.DataFrame(val_data)\n",
    "\n",
    "    # Xử lý test (nếu cần)\n",
    "    test_images_list = []\n",
    "    if os.path.exists(test_images):\n",
    "        for img_file in os.listdir(test_images):\n",
    "            test_images_list.append(os.path.join(test_images, img_file))\n",
    "\n",
    "    # Chia nhỏ tập train và val\n",
    "    train_df['caption'] = train_df['caption'].apply(preprocess_caption)\n",
    "    val_df['caption'] = val_df['caption'].apply(preprocess_caption)\n",
    "\n",
    "    return train_df, val_df, test_images_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_transform=None, tokenizer=None, max_length=30):\n",
    "        self.data = dataframe\n",
    "        self.image_transform = image_transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.iloc[idx, 0]\n",
    "        caption = self.data.iloc[idx, 1]\n",
    "\n",
    "        # Load and transform image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.image_transform(image)\n",
    "\n",
    "        # Tokenize the caption using tokenizer\n",
    "        tokenized = self.tokenizer(\n",
    "            caption,\n",
    "            padding=\"max_length\",  # Pad to max_length\n",
    "            truncation=True,       # Truncate to max_length\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"   # Return PyTorch tensors\n",
    "        )\n",
    "\n",
    "        # Extract input IDs\n",
    "        token_ids = tokenized[\"input_ids\"].squeeze(0)  # Remove batch dimension\n",
    "\n",
    "        return image, token_ids\n",
    "\n",
    "max_length = 40\n",
    "image_size = 224\n",
    "\n",
    "# Transforms\n",
    "# image_transforms = transforms.Compose([\n",
    "#     transforms.Resize((image_size, image_size)),\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),  # Resize ngắn nhất thành 256 để giữ tỷ lệ\n",
    "    transforms.CenterCrop(image_size),  # Cắt trung tâm ảnh để đảm bảo kích thước\n",
    "    transforms.ToTensor(),  # Chuyển thành tensor\n",
    "])\n",
    "\n",
    "# Đọc dữ liệu\n",
    "input_directory = \"/kaggle/input/flickr30k\"\n",
    "data_test = create_data_flickr(input_directory=input_directory)\n",
    "\n",
    "input_directory = \"/kaggle/input/coco-2017-dataset/coco2017\"\n",
    "data_train, data_val, test_images_coco = create_data_coco(input_directory=input_directory)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({\n",
    "    'bos_token': '<bos>',\n",
    "    'eos_token': '<eos>',\n",
    "    'pad_token': '<pad>'\n",
    "})\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dataset = ImageCaptionDataset(data_train, image_transform=image_transforms, tokenizer=tokenizer, max_length=max_length)\n",
    "val_dataset = ImageCaptionDataset(data_val, image_transform=image_transforms, tokenizer=tokenizer, max_length=max_length)\n",
    "test_dataset = ImageCaptionDataset(data_test, image_transform=image_transforms, tokenizer=tokenizer, max_length=max_length)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=os.cpu_count(), pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=os.cpu_count(), pin_memory=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=os.cpu_count(), pin_memory=True, prefetch_factor=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token Embedding\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.scale = sqrt(embedding_dim)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        return self.embedding(tokens) * self.scale\n",
    "        \n",
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_dim, dropout=0.1, maxlen=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(-torch.arange(0, embedding_dim, 2) * math.log(10000) / embedding_dim)\n",
    "        pos = torch.arange(0, maxlen).unsqueeze(1)\n",
    "        pos_embedding = torch.zeros(maxlen, embedding_dim)\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(0)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.pos_embedding[:, :x.size(1), :])\n",
    "\n",
    "class ImageCaptionModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        embedding_dim=768,\n",
    "        vocab_size=5000,\n",
    "        num_heads_decoder=8,\n",
    "        num_transformer_decoder_layers=6,\n",
    "        ffn_dim=2048,\n",
    "        dropout_decoder=0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the ImageCaptionModel.\n",
    "\n",
    "        Args:\n",
    "            encoder: Vision encoder model (e.g., Swin Transformer, ViT).\n",
    "            embedding_dim: Dimension of embeddings.\n",
    "            vocab_size: Size of the vocabulary.\n",
    "            num_heads_decoder: Number of attention heads in the decoder.\n",
    "            num_transformer_decoder_layers: Number of layers in the decoder.\n",
    "            ffn_dim: Dimension of the feed-forward network in the decoder.\n",
    "            dropout_decoder: Dropout rate in the decoder.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Swin Transformer Encoder\n",
    "        self.encoder = timm.create_model(model_name, pretrained=True, num_classes=0)\n",
    "        self.input_dim = self.encoder.num_features\n",
    "        self.linear_proj = nn.Linear(self.input_dim, embedding_dim)\n",
    "\n",
    "        # Token Embedding\n",
    "        self.token_embedding = TokenEmbedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Positional Encoding\n",
    "        self.positional_encoding = PositionalEncoding(embedding_dim, dropout=dropout_decoder)\n",
    "\n",
    "        # Transformer Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=num_heads_decoder,\n",
    "            dim_feedforward=ffn_dim,\n",
    "            dropout=dropout_decoder,\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_transformer_decoder_layers)\n",
    "\n",
    "        # Linear layer to project to vocabulary size\n",
    "        self.generator = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def encode(self, img):\n",
    "        \"\"\"\n",
    "        Encodes the image using the Swin Transformer encoder.\n",
    "        \"\"\"\n",
    "        img = img.to(next(self.encoder.parameters()).device) # Ensure data is on the same device as encoder\n",
    "        features = self.encoder.forward_features(img)  # [Batch, H, W, C]\n",
    "\n",
    "        # Permute to [Batch, C, H, W] for downstream processing\n",
    "        features = features.permute(0, 3, 1, 2)  # [Batch, C, H, W]\n",
    "\n",
    "        # Flatten spatial dimensions\n",
    "        features = features.flatten(2).permute(0, 2, 1)  # [Batch, seq_len, C]\n",
    "\n",
    "        # Project features to embedding dimension\n",
    "        memory = self.linear_proj(features)  # [Batch, seq_len, embed_dim]\n",
    "        return memory\n",
    "\n",
    "    def decode(self, tokens, memory, tgt_mask, tgt_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Decodes the target sequence using the Transformer decoder.\n",
    "\n",
    "        Args:\n",
    "            tokens: Target sequence tokens of shape [batch_size, tgt_seq_len].\n",
    "            memory: Encoded image features of shape [batch_size, num_patches, embedding_dim].\n",
    "            tgt_mask: Causal mask for the target sequence.\n",
    "            tgt_padding_mask: Padding mask for the target sequence.\n",
    "\n",
    "        Returns:\n",
    "            decoder_output: Decoded output of shape [batch_size, tgt_seq_len, embedding_dim].\n",
    "        \"\"\"\n",
    "        # Token embedding + positional encoding\n",
    "        tgt_emb = self.token_embedding(tokens)\n",
    "        tgt_emb = self.positional_encoding(tgt_emb)\n",
    "\n",
    "        decoder_output = self.decoder(\n",
    "            tgt_emb.permute(1, 0, 2),  # [tgt_seq_len, batch_size, embed_dim]\n",
    "            memory.permute(1, 0, 2),  # [num_patches, batch_size, embed_dim]\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=tgt_padding_mask,\n",
    "        )\n",
    "        return decoder_output\n",
    "\n",
    "    def forward(self, img, tokens, tgt_mask, tgt_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the model.\n",
    "\n",
    "        Args:\n",
    "            img: Input image tensor of shape [batch_size, 3, H, W].\n",
    "            tokens: Target sequence tokens of shape [batch_size, tgt_seq_len].\n",
    "            tgt_mask: Causal mask for the target sequence.\n",
    "            tgt_padding_mask: Padding mask for the target sequence.\n",
    "\n",
    "        Returns:\n",
    "            logits: Predicted logits of shape [batch_size, tgt_seq_len, vocab_size].\n",
    "        \"\"\"\n",
    "        memory = self.encode(img)\n",
    "        decoded = self.decode(tokens, memory, tgt_mask, tgt_padding_mask)\n",
    "        logits = self.generator(decoded.permute(1, 0, 2))  # Project to vocabulary\n",
    "        return logits\n",
    "\n",
    "    def generate_square_subsequent_mask(self, seq_len, device):\n",
    "      tgt_mask = (torch.triu(torch.ones((seq_len, seq_len), device=device)) == 1).transpose(0, 1)\n",
    "      tgt_mask = tgt_mask.float().masked_fill(tgt_mask == 0, float('-inf')).masked_fill(tgt_mask == 1, float(0.0))\n",
    "      return tgt_mask\n",
    "\n",
    "    def create_mask(self, tgt, pad_token_id, device):\n",
    "      tgt_seq_len = tgt.shape[1]\n",
    "      # Attention mask (float)\n",
    "      attention_mask = self.generate_square_subsequent_mask(tgt_seq_len, device=device)\n",
    "      # Padding mask (float)\n",
    "      padding_mask = (tgt == tokenizer.pad_token_id).float().to(device)\n",
    "      return attention_mask, padding_mask\n",
    "\n",
    "    def calculate_accuracy(self, logits, targets):\n",
    "        \"\"\"\n",
    "        Calculate token-level accuracy.\n",
    "\n",
    "        Args:\n",
    "            logits: Predicted logits of shape [batch_size, tgt_seq_len, vocab_size].\n",
    "            targets: Target tokens of shape [batch_size, tgt_seq_len].\n",
    "\n",
    "        Returns:\n",
    "            accuracy: Token-level accuracy.\n",
    "        \"\"\"\n",
    "        predictions = logits.argmax(dim=-1)\n",
    "        correct = (predictions == targets).float()\n",
    "        mask = (targets != 0).float()  # Exclude padding tokens\n",
    "        accuracy = (correct * mask).sum() / mask.sum()\n",
    "        return accuracy.item()\n",
    "\n",
    "    def train_epoch(self, dataloader, optimizer, criterion, tokenizer, device, clip_norm):\n",
    "        self.train()\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        scaler = GradScaler()\n",
    "    \n",
    "        for image_tensor, target_sequence in tqdm(dataloader, desc=\"Training\", unit=\"batch\"):\n",
    "            image_tensor, target_sequence = image_tensor.to(device), target_sequence.to(device)\n",
    "            decoder_input = target_sequence[:, :-1]\n",
    "            decoder_target = target_sequence[:, 1:]\n",
    "    \n",
    "            tgt_mask, tgt_padding_mask = self.create_mask(decoder_input, tokenizer.pad_token_id, device)\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            with autocast():\n",
    "                logits = self.forward(image_tensor, decoder_input, tgt_mask, tgt_padding_mask)\n",
    "                loss = criterion(logits.view(-1, logits.size(-1)), decoder_target.reshape(-1))\n",
    "                accuracy = self.calculate_accuracy(logits, decoder_target)  # Compute accuracy\n",
    "    \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            nn.utils.clip_grad_norm_(self.parameters(), clip_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "    \n",
    "            total_loss += loss.item()\n",
    "            total_accuracy += accuracy  # Accumulate accuracy\n",
    "    \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        avg_accuracy = total_accuracy / len(dataloader)\n",
    "        return avg_loss, avg_accuracy\n",
    "\n",
    "    def validate_epoch(self, dataloader, criterion, tokenizer, device):\n",
    "        self.eval()\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        num_batches = len(dataloader)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for image_tensor, target_sequence in tqdm(dataloader, desc=\"Validation\", unit=\"batch\"):\n",
    "                image_tensor, target_sequence = image_tensor.to(device), target_sequence.to(device)\n",
    "\n",
    "                # Prepare inputs for Transformer\n",
    "                tgt_input = target_sequence[:, :-1]\n",
    "                tgt_output = target_sequence[:, 1:]\n",
    "                tgt_mask, tgt_padding_mask = self.create_mask(tgt_input, tokenizer.pad_token_id, device)\n",
    "\n",
    "                logits = self(image_tensor, tgt_input, tgt_mask, tgt_padding_mask)\n",
    "                loss = criterion(logits.view(-1, logits.size(-1)), tgt_output.reshape(-1))\n",
    "\n",
    "                # Calculate metrics\n",
    "                total_loss += loss.item()\n",
    "                total_accuracy += model.calculate_accuracy(logits, tgt_output)\n",
    "\n",
    "        avg_loss = total_loss / num_batches\n",
    "        avg_accuracy = total_accuracy / num_batches\n",
    "        return avg_loss, avg_accuracy\n",
    "\n",
    "    def plot_loss_curves(self, train_losses, val_losses, train_accuracies, val_accuracies):\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(train_accuracies, label=\"Train accuracy\")\n",
    "        plt.plot(val_accuracies, label=\"Validation accuracy\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.title(\"Accuracy Curves\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(train_losses, label=\"Train Loss\")\n",
    "        plt.plot(val_losses, label=\"Validation Loss\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss Curves\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Đường dẫn đến file trong thư mục Input (mô hình đã tải lên trước)\n",
    "input_model_path = \"/kaggle/input/model/pytorch/default/1/swin_transformerdecoder.pth\"\n",
    "\n",
    "# Đường dẫn lưu file vào thư mục Output (Working)\n",
    "output_model_path = \"/kaggle/working/swin_transformerdecoder.pth\"\n",
    "\n",
    "# Sao chép mô hình từ Input sang Working để tiếp tục huấn luyện\n",
    "if os.path.exists(input_model_path):\n",
    "    print(\"Copying model from Input to Working directory...\")\n",
    "    shutil.copy(input_model_path, output_model_path)\n",
    "    print(f\"Model copied to {output_model_path}\")\n",
    "else:\n",
    "    print(\"No model found in Input. Training from scratch...\")\n",
    "\n",
    "# Initialize model, optimizer, and scheduler\n",
    "def initialize_model_and_optimizers(checkpoint_path, tokenizer, device,\n",
    "                                    model_name = \"swin_large_patch4_window7_224\",\n",
    "                                    embedding_dim=512,\n",
    "                                    vocab_size=None,\n",
    "                                    num_heads_decoder=8,\n",
    "                                    num_transformer_decoder_layers=6,\n",
    "                                    ffn_dim=2048,\n",
    "                                    dropout_decoder=0.1):\n",
    "    if vocab_size is None:\n",
    "        vocab_size = len(tokenizer)\n",
    "\n",
    "    # Kiểm tra nếu checkpoint tồn tại trong working directory\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(\"Checkpoint found. Loading model and optimizer...\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "        # Tạo mô hình từ checkpoint\n",
    "        model = ImageCaptionModel(\n",
    "            model_name=checkpoint[\"model_name\"],\n",
    "            embedding_dim=checkpoint[\"embedding_dim\"],\n",
    "            vocab_size=checkpoint[\"vocab_size\"],\n",
    "            num_heads_decoder=checkpoint[\"num_heads_decoder\"],\n",
    "            num_transformer_decoder_layers=checkpoint[\"num_transformer_decoder_layers\"],\n",
    "            ffn_dim=checkpoint[\"ffn_dim\"],\n",
    "            dropout_decoder=checkpoint[\"dropout_decoder\"],\n",
    "        ).to(device)\n",
    "\n",
    "        # Load trạng thái mô hình\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "        # Tạo optimizer và scheduler\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
    "        scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "\n",
    "        # Lấy thông tin từ checkpoint\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        train_losses = checkpoint[\"train_losses\"]\n",
    "        val_losses = checkpoint[\"val_losses\"]\n",
    "        train_accuracies = checkpoint[\"train_accuracies\"]\n",
    "        val_accuracies = checkpoint[\"val_accuracies\"]\n",
    "\n",
    "        print(f\"Resuming training from epoch {start_epoch}.\")\n",
    "    else:\n",
    "        print(\"No checkpoint found. Training from scratch...\")\n",
    "\n",
    "        # Tạo mô hình mới\n",
    "        model = ImageCaptionModel(\n",
    "            model_name=model_name,\n",
    "            embedding_dim=embedding_dim,\n",
    "            vocab_size=vocab_size,\n",
    "            num_heads_decoder=num_heads_decoder,\n",
    "            num_transformer_decoder_layers=num_transformer_decoder_layers,\n",
    "            ffn_dim=ffn_dim,\n",
    "            dropout_decoder=dropout_decoder,\n",
    "        ).to(device)\n",
    "\n",
    "        # Tạo optimizer và scheduler\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "        # Khởi tạo từ đầu\n",
    "        start_epoch = 1\n",
    "        train_losses, val_losses = [], []\n",
    "        train_accuracies, val_accuracies = [], []\n",
    "\n",
    "    return model, optimizer, scheduler, start_epoch, train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, start_epoch, train_loader, val_loader, tokenizer, device, checkpoint_path, epochs, clip_norm, train_losses, val_losses, train_accuracies, val_accuracies):\n",
    "\n",
    "    for epoch in range(start_epoch, epochs + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "        train_loss, train_accuracy = model.train_epoch(train_loader, optimizer, criterion, tokenizer, device, clip_norm)\n",
    "        val_loss, val_accuracy = model.validate_epoch(val_loader, criterion, tokenizer, device)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_accuracy:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Lưu checkpoint sau mỗi epoch vào thư mục Output (Working)\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"train_losses\": train_losses,\n",
    "            \"val_losses\": val_losses,\n",
    "            \"train_accuracies\": train_accuracies,\n",
    "            \"val_accuracies\": val_accuracies,\n",
    "            \"model_name\": \"swin_large_patch4_window7_224\",\n",
    "            \"embedding_dim\": 512,\n",
    "            \"vocab_size\": len(tokenizer),\n",
    "            \"num_heads_decoder\": 8,\n",
    "            \"num_transformer_decoder_layers\": 6,\n",
    "            \"ffn_dim\": 2048,\n",
    "            \"dropout_decoder\": 0.1,\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, scheduler, start_epoch, train_losses, val_losses, train_accuracies, val_accuracies = initialize_model_and_optimizers(\n",
    "    checkpoint_path=output_model_path, tokenizer=tokenizer, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = train_model(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    start_epoch=start_epoch,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    checkpoint_path=output_model_path,\n",
    "    epochs=1,\n",
    "    clip_norm=5.0,\n",
    "    train_losses=train_losses,\n",
    "    val_losses=val_losses,\n",
    "    train_accuracies=train_accuracies,\n",
    "    val_accuracies=val_accuracies,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm chuẩn hóa đầu vào\n",
    "def normalize_caption(caption):\n",
    "    \"\"\"Normalize captions by lowercasing, removing punctuation, and tokenizing.\"\"\"\n",
    "    caption = caption.lower()  # Lowercase\n",
    "    caption = caption.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    tokens = word_tokenize(caption)  # Tokenize\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Hàm loại bỏ các reference rỗng hoặc không hợp lệ\n",
    "def clean_references(reference_captions):\n",
    "    \"\"\"Remove empty or invalid references from a list.\"\"\"\n",
    "    return [caption for caption in reference_captions if caption.strip()]\n",
    "\n",
    "# Hàm sinh caption\n",
    "def generate_caption(model, image_tensor, tokenizer, max_length=35):\n",
    "    \"\"\"Generate captions for given images.\"\"\"\n",
    "    model.eval()\n",
    "    image_tensor = image_tensor.to(device)\n",
    "\n",
    "    with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):  # Mixed Precision\n",
    "        memory = model.encode(image_tensor)\n",
    "        batch_size = memory.shape[0]\n",
    "        generated_tokens = torch.full(\n",
    "            (batch_size, 1), tokenizer.bos_token_id, dtype=torch.long, device=device\n",
    "        )\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            tgt_mask = model.generate_square_subsequent_mask(generated_tokens.size(1), device=device)\n",
    "            tgt_emb = model.embedding(generated_tokens).permute(1, 0, 2)\n",
    "            decoder_output = model.decode(generated_tokens, memory, tgt_mask=tgt_mask)\n",
    "            next_token_logits = model.generator(decoder_output[-1, :, :])\n",
    "            next_token = next_token_logits.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "            if (next_token == tokenizer.eos_token_id).all():\n",
    "                break\n",
    "\n",
    "            generated_tokens = torch.cat([generated_tokens, next_token], dim=1)\n",
    "\n",
    "    captions = [\n",
    "        tokenizer.decode(generated_token.squeeze().tolist(), skip_special_tokens=True)\n",
    "        for generated_token in generated_tokens\n",
    "    ]\n",
    "\n",
    "    return [normalize_caption(caption) for caption in captions]  # Chuẩn hóa caption\n",
    "\n",
    "# Hàm đánh giá trên tập test\n",
    "def evaluate_on_test_set(model, test_loader, tokenizer):\n",
    "    \"\"\"Evaluate the model on the test dataset.\"\"\"\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Initialize scorers\n",
    "    \n",
    "    bleu_scorer = Bleu(4)\n",
    "    meteor_scorer = Meteor()\n",
    "    rouge_scorer = Rouge()\n",
    "    cider_scorer = Cider()\n",
    "\n",
    "    # Prepare gts and res\n",
    "    gts = {}\n",
    "    res = {}\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "        for batch_idx, (images, token_ids) in enumerate(tqdm(test_loader, desc=\"Evaluating test set\")):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            token_ids = [caption.to(device, non_blocking=True) for caption in token_ids]\n",
    "\n",
    "            # Generate captions\n",
    "            predicted_captions = generate_caption(model, images, tokenizer)\n",
    "\n",
    "            for idx, (predicted_caption, reference_tokens) in enumerate(zip(predicted_captions, token_ids)):\n",
    "                global_idx = batch_idx * test_loader.batch_size + idx\n",
    "\n",
    "                # Decode and normalize reference captions\n",
    "                reference_captions = [\n",
    "                    normalize_caption(tokenizer.decode(ref, skip_special_tokens=True)) for ref in reference_tokens\n",
    "                ]\n",
    "                reference_captions = clean_references(reference_captions)\n",
    "\n",
    "                if not reference_captions:\n",
    "                    continue\n",
    "\n",
    "                # Add to gts and res\n",
    "                gts[str(global_idx)] = reference_captions\n",
    "                res[str(global_idx)] = [predicted_caption]\n",
    "\n",
    "    # Calculate scores\n",
    "    bleu_scores, _ = bleu_scorer.compute_score(gts, res)\n",
    "    meteor_score, _ = meteor_scorer.compute_score(gts, res)\n",
    "    rouge_score, _ = rouge_scorer.compute_score(gts, res)\n",
    "    cider_score, _ = cider_scorer.compute_score(gts, res)\n",
    "\n",
    "    # Collect results\n",
    "    scores = {\n",
    "        \"BLEU-1\": bleu_scores[0],\n",
    "        \"BLEU-2\": bleu_scores[1],\n",
    "        \"BLEU-3\": bleu_scores[2],\n",
    "        \"BLEU-4\": bleu_scores[3],\n",
    "        \"METEOR\": meteor_score,\n",
    "        \"ROUGE-L\": rouge_score,\n",
    "        \"CIDEr\": cider_score\n",
    "    }\n",
    "\n",
    "    print(f\"Evaluation completed in {time.time() - start_time:.2f}s\")\n",
    "    return scores\n",
    "\n",
    "# Đánh giá\n",
    "test_scores = evaluate_on_test_set(model, test_loader, tokenizer)\n",
    "print(\"Test set evaluation scores:\", test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = [random.randint(0, len(data_test) - 1) for _ in range(5)]\n",
    "\n",
    "for i in rand:\n",
    "    row = data_test.iloc[i]\n",
    "    image_path = row['image']  # Đường dẫn ảnh\n",
    "    actual_caption = row['caption']  # Caption thực tế\n",
    "\n",
    "    # Load image\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    img = Image.open(image_path)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Actual: {actual_caption}\", fontsize=12, color=\"green\")\n",
    "    plt.show()\n",
    "\n",
    "    # Transform the image\n",
    "    img = img.convert(\"RGB\")\n",
    "    image_tensor = image_transforms(img).unsqueeze(0).to(device)\n",
    "\n",
    "    # Generate caption\n",
    "    predicted_caption = generate_caption(model, image_tensor, tokenizer, max_length=35)\n",
    "\n",
    "    # Fetch reference captions\n",
    "    reference_captions = data_test[data_test['image'] == image_path]['caption'].tolist()\n",
    "\n",
    "    # Display captions\n",
    "    print(\"Predicted Caption:\", predicted_caption)\n",
    "    if reference_captions:\n",
    "        print(\"Reference Captions:\")\n",
    "        for ref in reference_captions:\n",
    "            print(\"-\", ref)\n",
    "    else:\n",
    "        print(\"No reference captions found for this image.\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nguye\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\nguye\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nguye\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenizer đã được lưu vào thư mục 'tokenizer'\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer \n",
    "import os\n",
    "\n",
    "# Load tokenizer từ mô hình đã sử dụng\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Tạo thư mục nếu chưa có\n",
    "os.makedirs(\"tokenizer\", exist_ok=True)\n",
    "\n",
    "# Lưu tokenizer\n",
    "tokenizer.save_pretrained(\"tokenizer\")\n",
    "\n",
    "print(\"✅ Tokenizer đã được lưu vào thư mục 'tokenizer'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformersNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\nguye\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\nguye\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\nguye\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Downloading transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/9.7 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.8/9.7 MB 3.3 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.8/9.7 MB 3.3 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.3/9.7 MB 1.4 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 2.6/9.7 MB 2.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.7/9.7 MB 2.8 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.9/9.7 MB 3.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 5.8/9.7 MB 3.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 6.6/9.7 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.6/9.7 MB 3.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.7/9.7 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.4/9.7 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.4/9.7 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.7/9.7 MB 3.2 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp310-cp310-win_amd64.whl (274 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 1.3/2.4 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.4/2.4 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 4.0 MB/s eta 0:00:00\n",
      "Installing collected packages: regex, tokenizers, transformers\n",
      "Successfully installed regex-2024.11.6 tokenizers-0.21.0 transformers-4.48.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinTransformer(\n",
      "  (features): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (1): Permute()\n",
      "      (2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): SwinTransformerBlockV2(\n",
      "        (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "          (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=512, out_features=3, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (0): Linear(in_features=96, out_features=384, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=384, out_features=96, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): SwinTransformerBlockV2(\n",
      "        (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "          (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=512, out_features=3, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.013043478260869565, mode=row)\n",
      "        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (0): Linear(in_features=96, out_features=384, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=384, out_features=96, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): PatchMergingV2(\n",
      "      (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
      "      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): SwinTransformerBlockV2(\n",
      "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=512, out_features=6, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.02608695652173913, mode=row)\n",
      "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (0): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): SwinTransformerBlockV2(\n",
      "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=512, out_features=6, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.03913043478260869, mode=row)\n",
      "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (0): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): PatchMergingV2(\n",
      "      (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
      "      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): SwinTransformerBlockV2(\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.05217391304347826, mode=row)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): SwinTransformerBlockV2(\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.06521739130434782, mode=row)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): SwinTransformerBlockV2(\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.07826086956521738, mode=row)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): SwinTransformerBlockV2(\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.09130434782608696, mode=row)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): SwinTransformerBlockV2(\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.10434782608695652, mode=row)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): SwinTransformerBlockV2(\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.11739130434782608, mode=row)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): SwinTransformerBlockV2(\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): SwinTransformerBlockV2(\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.14347826086956522, mode=row)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): SwinTransformerBlockV2(\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.15652173913043477, mode=row)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): SwinTransformerBlockV2(\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.16956521739130434, mode=row)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): SwinTransformerBlockV2(\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1826086956521739, mode=row)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): SwinTransformerBlockV2(\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1956521739130435, mode=row)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (12): SwinTransformerBlockV2(\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.20869565217391303, mode=row)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (13): SwinTransformerBlockV2(\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.2217391304347826, mode=row)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (14): SwinTransformerBlockV2(\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.23478260869565215, mode=row)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (15): SwinTransformerBlockV2(\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.24782608695652175, mode=row)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (16): SwinTransformerBlockV2(\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.2608695652173913, mode=row)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (17): SwinTransformerBlockV2(\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.27391304347826084, mode=row)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): PatchMergingV2(\n",
      "      (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
      "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): SwinTransformerBlockV2(\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=512, out_features=24, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.28695652173913044, mode=row)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): SwinTransformerBlockV2(\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftedWindowAttentionV2(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (cpb_mlp): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=512, out_features=24, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.3, mode=row)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (permute): Permute()\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "model = models.swin_v2_s(weights=\"IMAGENET1K_V1\")\n",
    "print(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
